---
title: "projectC"
author: "Mathi Manavalan"
date: "5/7/2020"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries
```{r message=FALSE}
#library(psych)
library(tidyverse)
library(Hmisc)
library(caret)
```


## Data Import and Cleaning

I am using data from the **The 1996 Topical Module: Emotions** that I am getting from the GSS site (http://gss.norc.org/get-the-data/spss). I chose this data set as it is an extremely large, publicly available data set on a variety of psychological queries. From this specific data set, I would like to look at the questions which ask participants the number of days (within the last 7 days) that they've felt a certain emotion. From this, I would like to see if I can predict how many days they felt 'at ease' (which is one of the questions) from the number of days they felt all the other emotions ('couldn't shake the blues', 'outraged', 'happy', 'sad', 'ashamed', 'excited', 'lonely', 'fearful', 'overjoyed', 'worried', 'anxious', 'restless', 'mad', 'angry', 'embarrassed', and 'proud'). I have purposefully excluded certain emotions ('contented' and 'calm') that seemed to be synonyms for 'ease' (as they are highly correlated with each other). So my overall question would be, can the number of days that certain emotions were felt predict the number of days there was a feeling of being at-ease? 


For this question, machine learning would be an appropriate approach as I want to be able to generalize the findings of my question to other similar data sets (data sets that have been generated in the same way). So my question is not specific to this data set as I am not just trying to find a model that appropriately models this one data set, but I am prioritizing generalizability. Here, I want to be able to predict out-of-sample as strongly as I can and equally well in the future.


I began by importing this data set and selecting those variables (I have identified above). Then I cleaned the data set such that I do not have any observations which do not have a valid response for ATEASE, as ATEASE is what I am trying to predict. Then, I removed all observations that have invalid responses for all of the independent variables (turns out, there are no such observations). Then I checked the number of invalid responses I have which turned out to be 118. With the total number of observations being 1443, we can simply proceed as is. 

```{r message=FALSE, warning=FALSE}
data <- as_tibble(spss.get("data/GSS1996.sav", use.value.labels=TRUE)) 


import <- data %>% 
  select(SHAKEBLU, OUTRAGED, HAPFEEL, SAD, ASHAMED, EXCITED, LONELY, FEARFUL, OVRJOYED, WORRIED, ANXIOUS, RESTLESS, MADAT, ATEASE, ANGRY, EMBARRSS, PROUD) %>% 
  filter_all(any_vars(!is.na(.))) %>% 
  drop_na(ATEASE) %>% 
  mutate_all(. %>% 
               as.numeric()
             )

# following line gets rid of data that have all DVs missing (turns out, there are none!)
clean <- import[!(rowSums(is.na(import)) == 16),]

#table(clean$ATEASE)

# only have 118 NAs in all 1443 cleaned observations, so we can proceed
sum(is.na(clean)) 

```

note: responses are counts with a range of 0 days to 7 days, whereas the data has a range of responses from 1 to 8, which is fine, but should be kept in mind


## Analysis

I started by looking at the Classic ML Cheat Sheet to guide me through the process of choosing appropriate analyses. Following the path on the cheat sheet, I know that I have >50 samples, I am predicting a quantity (more specifically, a count), I have fewer than 100K samples, and I have a few features that should be considered as important. This leads me to look consider Lasso or ElasticNet. Depending on my interpretation of what the cheat sheet means by 'few features', I could have also considered Ridge regression.  Both Lasso and Ridge regression are basically the same, except for the way each penalizes really large predictor coefficients - the weights calculated in Lasso are sum of the absolute values and in Ridge, its the sum of the squares of the weights. Elastic net basically adds a new parameter, alpha, which indicates the blend of the two types of penalties (Lasso and Ridge). So if alpha is 1, then it is calculating the penalities 100% the Lasso way and if alpha is 0, it's 100% Ridge. 

With this knowledge, I decided to use the Elastic Net ML model, as that, in a way, encompasses Lasso and Ridge. And to deal with the issue of running into local minima (in minimizing the results of the cost function), I decided to run a 10-fold cross validation. 

```{r}
#choosing 200 as a holdout size, as we have 1443 clean observations so 200 gives us a workable proportion of the total data
holdout_indices <- sample(nrow(clean), 200)
train_indices <- (1:nrow(clean))[-holdout_indices]


train_tbl <- clean[train_indices,]
test_tbl <- clean[holdout_indices,]


elasticModel <- train(
  ATEASE ~ .,
  train_tbl,
  method = "glmnet",
  preProcess = c("center", "scale", "zv", "knnImpute"),
  trControl = trainControl(
    method = "cv",
    number = 10,
    verboseIter = TRUE
  ),
  na.action = na.pass
)

elasticModel

cor(predict(elasticModel, test_tbl, na.action = na.pass), test_tbl$ATEASE)^2

```

We can see that the correlation is alright, with an R^2 of 0.27. So I wouldn't say that the model performs exceptionally well out-of-sample, but it's decent. (I had also run this model on the same data set, except I also included the two emotion variables 'contented' and 'calm', and not surprisingly, I got a much larger correlation value).

We can also see that the optimized values for alpha is 1 and for lambda is 0.023. From this, we know that this model was actually 100% Lasso. Looking at the lambda values, we can see that the penalty value chosen was neither the smallest nor the largest, so an extreme penalty was not needed. 

